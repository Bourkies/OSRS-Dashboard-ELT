# OSRS Dashboard ET Pipeline

This project contains the ET (Extract, Transform) pipeline for a custom clan dashboard. It is designed to be run as a Docker container, fetching data from Discord, processing it with a highly configurable Python script, and preparing it for use in a dashboard.

This pipeline is intended to be deployed on a server (like a Raspberry Pi) and run on a schedule (e.g., using a cron job).
Please note AI has been extensively used in the creation of this project

## Features

* **Discord Integration**: Fetches message history from a specified Discord channel.
* **Advanced Regex Parsing**: Uses TOML configuration files to define complex regex patterns for parsing different message types (e.g., chat, drops, achievements).
* **Local Database**: Uses a local SQLite database to store all raw, parsed, and transformed data, keeping the setup simple and self-contained.
* **Automated Username & Record Management**:
    * Combine multiple historical usernames into a single current name.
    * Blacklist specific personal bests (or all PBs for a user) to maintain leaderboard integrity.
* **Robust Deployment**: Designed to run in a Docker container with a clean, repeatable setup process.
* **Simple Updates**: A `git pull` and `docker-compose build` is all that's needed to deploy new code without affecting your data or configurations.

## Core Dependency: RuneLite Plugin

This script is specifically designed to parse messages generated by the [**Clan Chat Webhook**](https://runelite.net/plugin-hub/show/clan-chat-webhook) plugin for the RuneLite client.

**This is a hard requirement.** For the parsing to work correctly, you must:

1.  Install the "Clan Chat Webhook" plugin from the RuneLite Plugin Hub.
2.  Configure the plugin to send messages to a specific webhook in your Discord server.
3.  The Discord channel that this script reads from (specified in your `secrets.toml`) **should exclusively contain messages from this webhook.** Other chat or bot messages in this channel will cause parsing errors.

---

## Project Structure

The project is organized to separate the core application code from user-generated data and private configurations.

* **/src**: Contains all Python source code and the `.example.toml` configuration templates. This is the code that gets updated via `git pull`.
* **/data**: (Git Ignored) Stores the local SQLite databases. This directory is mounted as a Docker volume to persist data across container runs.
* **/logs**: (Git Ignored) Contains detailed log files for each script run. Also mounted as a Docker volume.
* **/summaries**: (Git Ignored) Contains summary text files for each script run. Also mounted as a Docker volume.
* `Dockerfile`: Instructions to build the ETL Docker image.
* `.gitignore`: Specifies which files and directories Git should ignore, protecting your data and secrets.

---

## Deployment & Setup on a Server (e.g., Raspberry Pi)

Follow these steps to set up and run the ETL pipeline.

### Prerequisites

* [Git](https://git-scm.com/downloads/)
* [Docker and Docker Compose](https://docs.docker.com/engine/install/) (Docker Desktop for Windows/Mac includes Compose)

### 1. Clone the Repository

Clone this repository onto your server into a folder of your choosing. This folder will be your project root.

```bash
git clone <your-github-repo-url> <your-project-folder-name>
cd <your-project-folder-name>
```

### 2. Create Persistent Directories

The `data`, `logs`, and `summaries` directories are intentionally excluded from Git. You need to create them manually in the project root on your server the first time. This ensures that your data is never accidentally deleted.

```bash
mkdir data
mkdir logs
mkdir summaries
```

### 3. Configure Your Environment

Your secrets and configurations are kept out of Git for security and flexibility. You will create your local configs by copying the provided templates.

1.  Navigate to the `src` directory: `cd src`
2.  Copy the example files to create your local configuration:
    ```bash
    cp secrets.example.toml secrets.toml
    cp config.example.toml config.toml
    cp historical_collection_logs.example.toml historical_collection_logs.toml
    cp historical_personal_bests.example.toml historical_personal_bests.toml
    ```
3.  Open each of the new `.toml` files (e.g., `nano secrets.toml`) and fill in your actual values. This is where you'll add your Discord bot token, channel IDs, webhook URL, username mappings, and historical data.

### 4. Build the Docker Image

Navigate back to the project's root directory (`cd ..`) and use Docker Compose to build the service image.

```bash
docker build -t osrs-etl .
```

---

## Running the ETL

### Manual Run

You can trigger a single ETL run at any time with the following command. This is great for testing.

```bash
docker run --rm -v "$(pwd)/data:/app/data" -v "$(pwd)/logs:/app/logs" -v "$(pwd)/summaries:/app/summaries" -v "$(pwd)/src/secrets.toml:/app/src/secrets.toml" -v "$(pwd)/src/config.toml:/app/src/config.toml" -v "$(pwd)/src/historical_collection_logs.toml:/app/src/historical_collection_logs.toml" -v "$(pwd)/src/historical_personal_bests.toml:/app/src/historical_personal_bests.toml" osrs-etl
```

### Automating with Cron

To run the pipeline automatically, you can create a cron job.

1.  Open the cron table for editing: `crontab -e`
2.  Add a line to define the schedule. This example runs the ETL every 15 minutes:
    ```crontab
    */15 * * * * cd /path/to/your/<your-project-folder-name> && docker run --rm -v "$(pwd)/data:/app/data" -v "$(pwd)/logs:/app/logs" -v "$(pwd)/summaries:/app/summaries" -v "$(pwd)/src/secrets.toml:/app/src/secrets.toml" -v "$(pwd)/src/config.toml:/app/src/config.toml" -v "$(pwd)/src/historical_collection_logs.toml:/app/src/historical_collection_logs.toml" -v "$(pwd)/src/historical_personal_bests.toml:/app/src/historical_personal_bests.toml" osrs-etl >> /path/to/your/<your-project-folder-name>/logs/cron.log 2>&1
    ```
    **Important:**
    * Replace `/path/to/your/<your-project-folder-name>` with the actual absolute path to your project.
    * The `>> ... 2>&1` part redirects all output to a `cron.log` file, which is useful for debugging.

---

## Updating the Script

This workflow makes updates incredibly simple.

1.  **Pull the latest code** from your GitHub repository:
    ```bash
    cd /path/to/your/<your-project-folder-name>
    git pull
    ```
2.  **Rebuild the Docker image** with the new code:
    ```bash
    docker build -t osrs-etl .
    ```

That's it! Your cron job will automatically start using the new version on its next run. Because your `data`, `logs`, `summaries`, and all your `.toml` configuration files are managed by `.gitignore` and Docker volumes, they are completely safe and will not be affected by the `git pull` command.
